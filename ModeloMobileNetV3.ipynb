{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_MRIha2_Fpo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52624bd"
      },
      "source": [
        "### Training Code\n",
        "\n",
        "Paste your Python training code here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a09190f0",
        "outputId": "9b722600-b5fb-4a89-d658-0bf0493e0cc5"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configuración\n",
        "IMG_SIZE = 224  # Tamaño de entrada para MobileNet\n",
        "BATCH_SIZE = 16  # Aumentado para mejor generalización\n",
        "EPOCHS = 150  # Más épocas para aprendizaje profundo\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# Rutas\n",
        "# ¡ADVERTENCIA! Estas rutas deben ser actualizadas para el entorno de Colab.\n",
        "# DATA_DIR = r\"C:\\Users\\Eduardo\\Documents\\Maestria\\TFM\\Proyecto\\B. Disease Grading\\B. Disease Grading\\1. Original Images\\a. Training Set\"\n",
        "# LABELS_FILE = r\"C:\\Users\\Eduardo\\Documents\\Maestria\\TFM\\Proyecto\\B. Disease Grading\\B. Disease Grading\\2. Groundtruths\\a. IDRiD_Disease Grading_Training Labels.csv\"\n",
        "\n",
        "# EJEMPLO: Si subes tus archivos a una carpeta 'data' en Colab\n",
        "# Puedes montar Google Drive o subir directamente los archivos a /content/\n",
        "DATA_DIR = \"/content/sample_data/a. Training Set\" # Ruta de las imágenes actualizada\n",
        "LABELS_FILE = \"/content/sample_data/a. IDRiD_Disease Grading_Training Labels.csv\" # Ruta actualizada\n",
        "\n",
        "MODEL_SAVE_PATH = \"models\"\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Clase para cargar y preprocesar los datos\"\"\"\n",
        "\n",
        "    def __init__(self, labels_file, data_dir, img_size=224):\n",
        "        self.labels_file = labels_file\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        self.df = None\n",
        "\n",
        "    def load_labels(self):\n",
        "        \"\"\"Carga el archivo CSV con las etiquetas\"\"\"\n",
        "        # Intentar leer como CSV primero, si falla intentar Excel\n",
        "        try:\n",
        "            self.df = pd.read_csv(self.labels_file)\n",
        "        except:\n",
        "            self.df = pd.read_excel(self.labels_file)\n",
        "\n",
        "        print(f\"Dataset cargado: {len(self.df)} imágenes\")\n",
        "        print(f\"\\nColumnas: {self.df.columns.tolist()}\")\n",
        "\n",
        "        # Detectar el nombre de la columna de grado de retinopatía\n",
        "        grade_column = None\n",
        "        for col in self.df.columns:\n",
        "            if 'retinopathy' in col.lower() and 'grade' in col.lower():\n",
        "                grade_column = col\n",
        "                break\n",
        "\n",
        "        if grade_column is None:\n",
        "            # Buscar columnas alternativas\n",
        "            possible_columns = [col for col in self.df.columns if 'grade' in col.lower() or 'label' in col.lower()]\n",
        "            if possible_columns:\n",
        "                grade_column = possible_columns[0]\n",
        "            else:\n",
        "                print(\"ERROR: No se encontró la columna de grado de retinopatía\")\n",
        "                print(f\"Columnas disponibles: {self.df.columns.tolist()}\")\n",
        "                return None\n",
        "\n",
        "        self.grade_column = grade_column\n",
        "        print(f\"\\nUsando columna: '{grade_column}'\")\n",
        "        print(f\"\\nDistribución de clases:\")\n",
        "        print(self.df[grade_column].value_counts().sort_index())\n",
        "        return self.df\n",
        "\n",
        "    def load_and_preprocess_image(self, image_name):\n",
        "        \"\"\"Carga y preprocesa una imagen\"\"\"\n",
        "        # Si el nombre ya tiene extensión, usarlo directamente\n",
        "        if any(image_name.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png']):\n",
        "            image_path = os.path.join(self.data_dir, image_name)\n",
        "            if os.path.exists(image_path):\n",
        "                img = cv2.imread(image_path)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "                img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "                return img\n",
        "\n",
        "        # Buscar la imagen con diferentes extensiones\n",
        "        for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:\n",
        "            image_path = os.path.join(self.data_dir, image_name + ext)\n",
        "            if os.path.exists(image_path):\n",
        "                break\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"No se encontró la imagen: {image_name}\")\n",
        "\n",
        "        # Leer imagen\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Redimensionar\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "\n",
        "        # Normalizar a [-1, 1] (preprocesamiento de MobileNet)\n",
        "        img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def prepare_data(self, test_size=0.2, val_size=0.1):\n",
        "        \"\"\"Prepara los datos para entrenamiento\"\"\"\n",
        "        if self.df is None:\n",
        "            self.load_labels()\n",
        "\n",
        "        # Detectar el nombre de la columna de imagen\n",
        "        image_column = None\n",
        "        for col in self.df.columns:\n",
        "            if 'image' in col.lower() and 'name' in col.lower():\n",
        "                image_column = col\n",
        "                break\n",
        "\n",
        "        if image_column is None:\n",
        "            # Buscar la primera columna que parezca contener nombres de archivos\n",
        "            for col in self.df.columns:\n",
        "                if self.df[col].dtype == 'object':\n",
        "                    image_column = col\n",
        "                    break\n",
        "\n",
        "        if image_column is None:\n",
        "            image_column = self.df.columns[0]\n",
        "\n",
        "        print(f\"\\nUsando columna de imágenes: '{image_column}'\")\n",
        "\n",
        "        # Cargar todas las imágenes\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        print(\"\\nCargando imágenes...\")\n",
        "        for idx, row in self.df.iterrows():\n",
        "            try:\n",
        "                img = self.load_and_preprocess_image(row[image_column])\n",
        "                images.append(img)\n",
        "                labels.append(row[self.grade_column])\n",
        "\n",
        "                if (idx + 1) % 10 == 0:\n",
        "                    print(f\"Procesadas {idx + 1}/{len(self.df)} imágenes\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error cargando {row[image_column]}: {str(e)}\")\n",
        "\n",
        "        images = np.array(images)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        print(f\"\\nImágenes cargadas: {len(images)}\")\n",
        "        print(f\"Shape de las imágenes: {images.shape}\")\n",
        "\n",
        "        # Split en train, validation y test\n",
        "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "            images, labels, test_size=test_size, random_state=42, stratify=labels\n",
        "        )\n",
        "\n",
        "        val_size_adjusted = val_size / (1 - test_size)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_val, y_train_val, test_size=val_size_adjusted,\n",
        "            random_state=42, stratify=y_train_val\n",
        "        )\n",
        "\n",
        "        print(f\"\\nDatos divididos:\")\n",
        "        print(f\"  Train: {len(X_train)} imágenes\")\n",
        "        print(f\"  Validation: {len(X_val)} imágenes\")\n",
        "        print(f\"  Test: {len(X_test)} imágenes\")\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "class MobileNetClassifier:\n",
        "    \"\"\"Modelo MobileNet para clasificación de retinopatía diabética\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=5, img_size=224):\n",
        "        self.num_classes = num_classes\n",
        "        self.img_size = img_size\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Construye el modelo usando MobileNetV2 con transfer learning\"\"\"\n",
        "        # Cargar MobileNetV2 pre-entrenado en ImageNet\n",
        "        base_model = MobileNetV2(\n",
        "            input_shape=(self.img_size, self.img_size, 3),\n",
        "            include_top=False,\n",
        "            weights='imagenet'\n",
        "        )\n",
        "\n",
        "        # Congelar las primeras capas del modelo base\n",
        "        base_model.trainable = False\n",
        "\n",
        "        # Añadir capas de clasificación personalizadas\n",
        "        x = base_model.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dense(512, activation='relu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        x = Dense(256, activation='relu')(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        predictions = Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        self.model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "        print(\"\\n=== Arquitectura del Modelo ===\")\n",
        "        print(f\"Capas del modelo base (MobileNetV2): {len(base_model.layers)}\")\n",
        "        print(f\"Total de capas: {len(self.model.layers)}\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def compile_model(self, learning_rate=0.0001):\n",
        "        \"\"\"Compila el modelo\"\"\"\n",
        "        self.model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "        )\n",
        "        print(\"\\nModelo compilado con éxito\")\n",
        "\n",
        "    def unfreeze_layers(self, num_layers=20):\n",
        "        \"\"\"Descongela las últimas capas del modelo base para fine-tuning\"\"\"\n",
        "        # Encontrar la capa MobileNetV2 en el modelo\n",
        "        base_model = None\n",
        "        for layer in self.model.layers:\n",
        "            if isinstance(layer, tf.keras.Model) and 'mobilenet' in layer.name.lower():\n",
        "                base_model = layer\n",
        "                break\n",
        "\n",
        "        if base_model is None:\n",
        "            print(\"Advertencia: No se encontró MobileNetV2, descongelando todo el modelo\")\n",
        "            self.model.trainable = True\n",
        "            return\n",
        "\n",
        "        base_model.trainable = True\n",
        "\n",
        "        # Congelar todas excepto las últimas num_layers\n",
        "        for layer in base_model.layers[:-num_layers]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        print(f\"\\nDescongeladas las últimas {num_layers} capas para fine-tuning\")\n",
        "\n",
        "\n",
        "def create_data_augmentation():\n",
        "    \"\"\"Crea un generador de data augmentation\"\"\"\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        zoom_range=0.2,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    return train_datagen\n",
        "\n",
        "\n",
        "def plot_training_history(history, save_path='training_history.png'):\n",
        "    \"\"\"Visualiza el historial de entrenamiento\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Accuracy\n",
        "    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Loss\n",
        "    ax2.plot(history.history['loss'], label='Train Loss')\n",
        "    ax2.plot(history.history['val_loss'], label='Val Loss')\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"\\nGráfica guardada en: {save_path}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Función principal de entrenamiento\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ENTRENAMIENTO: Clasificación de Retinopatía Diabética\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Crear directorios necesarios\n",
        "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "    # 1. Cargar datos\n",
        "    print(\"\\n[1/5] Cargando datos...\")\n",
        "    data_loader = DataLoader(LABELS_FILE, DATA_DIR, IMG_SIZE)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = data_loader.prepare_data()\n",
        "\n",
        "    # Calcular pesos de clase para manejar desbalance\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "    print(f\"\\nPesos de clase (para desbalance): {class_weight_dict}\")\n",
        "\n",
        "    # 2. Crear modelo\n",
        "    print(\"\\n[2/5] Creando modelo MobileNet...\")\n",
        "    classifier = MobileNetClassifier(num_classes=5, img_size=IMG_SIZE)\n",
        "    model = classifier.build_model()\n",
        "    classifier.compile_model(learning_rate=LEARNING_RATE)\n",
        "\n",
        "    # 3. Configurar callbacks\n",
        "    print(\"\\n[3/5] Configurando callbacks...\")\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(\n",
        "            os.path.join(MODEL_SAVE_PATH, 'best_model.h5'),\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # 4. Entrenar modelo (Fase 1: Feature extraction)\n",
        "    print(\"\\n[4/5] Entrenando modelo - Fase 1: Feature Extraction...\")\n",
        "    print(f\"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "    history1 = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=EPOCHS // 2,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 5. Fine-tuning\n",
        "    print(\"\\n[5/5] Entrenando modelo - Fase 2: Fine-tuning...\")\n",
        "    classifier.unfreeze_layers(num_layers=30)\n",
        "    classifier.compile_model(learning_rate=LEARNING_RATE / 10)\n",
        "\n",
        "    history2 = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=EPOCHS // 2,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weight_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Combinar historiales\n",
        "    for key in history1.history.keys():\n",
        "        history1.history[key].extend(history2.history[key])\n",
        "\n",
        "    # 6. Evaluar en test set\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EVALUACIÓN EN TEST SET\")\n",
        "    print(\"=\" * 60)\n",
        "    test_loss, test_accuracy, _ = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Predicciones en test\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Matriz de confusión\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    print(\"\\n=== Classification Report ===\")\n",
        "    print(classification_report(y_test, y_pred_classes,\n",
        "                                target_names=[f'Grade {i}' for i in range(5)]))\n",
        "\n",
        "    print(\"\\n=== Confusion Matrix ===\")\n",
        "    cm = confusion_matrix(y_test, y_pred_classes)\n",
        "    print(cm)\n",
        "\n",
        "    # Guardar resultados\n",
        "    plot_training_history(history1, 'training_history.png')\n",
        "    model.save(os.path.join(MODEL_SAVE_PATH, 'final_model.h5'))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ENTRENAMIENTO COMPLETADO\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Modelo guardado en: {MODEL_SAVE_PATH}/final_model.h5\")\n",
        "    print(f\"Mejor modelo en: {MODEL_SAVE_PATH}/best_model.h5\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ENTRENAMIENTO: Clasificación de Retinopatía Diabética\n",
            "============================================================\n",
            "\n",
            "[1/5] Cargando datos...\n",
            "Dataset cargado: 413 imágenes\n",
            "\n",
            "Columnas: ['Image name', 'Retinopathy grade', 'Risk of macular edema ', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11']\n",
            "\n",
            "Usando columna: 'Retinopathy grade'\n",
            "\n",
            "Distribución de clases:\n",
            "Retinopathy grade\n",
            "0    134\n",
            "1     20\n",
            "2    136\n",
            "3     74\n",
            "4     49\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Usando columna de imágenes: 'Image name'\n",
            "\n",
            "Cargando imágenes...\n",
            "Procesadas 10/413 imágenes\n",
            "Procesadas 20/413 imágenes\n",
            "Procesadas 30/413 imágenes\n",
            "Procesadas 40/413 imágenes\n",
            "Procesadas 50/413 imágenes\n",
            "Procesadas 60/413 imágenes\n",
            "Procesadas 70/413 imágenes\n",
            "Procesadas 80/413 imágenes\n",
            "Procesadas 90/413 imágenes\n",
            "Procesadas 100/413 imágenes\n",
            "Procesadas 110/413 imágenes\n",
            "Procesadas 120/413 imágenes\n",
            "Procesadas 130/413 imágenes\n",
            "Procesadas 140/413 imágenes\n",
            "Procesadas 150/413 imágenes\n",
            "Procesadas 160/413 imágenes\n",
            "Procesadas 170/413 imágenes\n",
            "Procesadas 180/413 imágenes\n",
            "Procesadas 190/413 imágenes\n",
            "Procesadas 200/413 imágenes\n",
            "Procesadas 210/413 imágenes\n",
            "Procesadas 220/413 imágenes\n",
            "Procesadas 230/413 imágenes\n",
            "Procesadas 240/413 imágenes\n",
            "Procesadas 250/413 imágenes\n",
            "Procesadas 260/413 imágenes\n",
            "Procesadas 270/413 imágenes\n",
            "Procesadas 280/413 imágenes\n",
            "Procesadas 290/413 imágenes\n",
            "Procesadas 300/413 imágenes\n",
            "Procesadas 310/413 imágenes\n",
            "Procesadas 320/413 imágenes\n",
            "Procesadas 330/413 imágenes\n",
            "Procesadas 340/413 imágenes\n",
            "Procesadas 350/413 imágenes\n",
            "Procesadas 360/413 imágenes\n",
            "Procesadas 370/413 imágenes\n",
            "Procesadas 380/413 imágenes\n",
            "Procesadas 390/413 imágenes\n",
            "Procesadas 400/413 imágenes\n",
            "Procesadas 410/413 imágenes\n",
            "\n",
            "Imágenes cargadas: 413\n",
            "Shape de las imágenes: (413, 224, 224, 3)\n",
            "\n",
            "Datos divididos:\n",
            "  Train: 288 imágenes\n",
            "  Validation: 42 imágenes\n",
            "  Test: 83 imágenes\n",
            "\n",
            "Pesos de clase (para desbalance): {0: np.float64(0.6193548387096774), 1: np.float64(4.114285714285714), 2: np.float64(0.6063157894736843), 3: np.float64(1.1076923076923078), 4: np.float64(1.6941176470588235)}\n",
            "\n",
            "[2/5] Creando modelo MobileNet...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "=== Arquitectura del Modelo ===\n",
            "Capas del modelo base (MobileNetV2): 154\n",
            "Total de capas: 160\n",
            "\n",
            "Modelo compilado con éxito\n",
            "\n",
            "[3/5] Configurando callbacks...\n",
            "\n",
            "[4/5] Entrenando modelo - Fase 1: Feature Extraction...\n",
            "Epochs: 50, Batch Size: 16\n",
            "Epoch 1/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519ms/step - accuracy: 0.1969 - loss: 2.1554 - sparse_categorical_accuracy: 0.1969\n",
            "Epoch 1: val_accuracy improved from -inf to 0.16667, saving model to models/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 808ms/step - accuracy: 0.1984 - loss: 2.1542 - sparse_categorical_accuracy: 0.1984 - val_accuracy: 0.1667 - val_loss: 1.5407 - val_sparse_categorical_accuracy: 0.1667 - learning_rate: 1.0000e-04\n",
            "Epoch 2/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493ms/step - accuracy: 0.2247 - loss: 1.6217 - sparse_categorical_accuracy: 0.2247\n",
            "Epoch 2: val_accuracy improved from 0.16667 to 0.30952, saving model to models/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 654ms/step - accuracy: 0.2268 - loss: 1.6250 - sparse_categorical_accuracy: 0.2268 - val_accuracy: 0.3095 - val_loss: 1.4507 - val_sparse_categorical_accuracy: 0.3095 - learning_rate: 1.0000e-04\n",
            "Epoch 3/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504ms/step - accuracy: 0.2440 - loss: 1.6492 - sparse_categorical_accuracy: 0.2440\n",
            "Epoch 3: val_accuracy improved from 0.30952 to 0.42857, saving model to models/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 688ms/step - accuracy: 0.2443 - loss: 1.6488 - sparse_categorical_accuracy: 0.2443 - val_accuracy: 0.4286 - val_loss: 1.3621 - val_sparse_categorical_accuracy: 0.4286 - learning_rate: 1.0000e-04\n",
            "Epoch 4/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501ms/step - accuracy: 0.3786 - loss: 1.3854 - sparse_categorical_accuracy: 0.3786\n",
            "Epoch 4: val_accuracy did not improve from 0.42857\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 601ms/step - accuracy: 0.3790 - loss: 1.3842 - sparse_categorical_accuracy: 0.3790 - val_accuracy: 0.3571 - val_loss: 1.3310 - val_sparse_categorical_accuracy: 0.3571 - learning_rate: 1.0000e-04\n",
            "Epoch 5/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511ms/step - accuracy: 0.3475 - loss: 1.5664 - sparse_categorical_accuracy: 0.3475\n",
            "Epoch 5: val_accuracy did not improve from 0.42857\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 593ms/step - accuracy: 0.3465 - loss: 1.5612 - sparse_categorical_accuracy: 0.3465 - val_accuracy: 0.3810 - val_loss: 1.3044 - val_sparse_categorical_accuracy: 0.3810 - learning_rate: 1.0000e-04\n",
            "Epoch 6/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572ms/step - accuracy: 0.4217 - loss: 1.3887 - sparse_categorical_accuracy: 0.4217\n",
            "Epoch 6: val_accuracy improved from 0.42857 to 0.47619, saving model to models/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 671ms/step - accuracy: 0.4220 - loss: 1.3857 - sparse_categorical_accuracy: 0.4220 - val_accuracy: 0.4762 - val_loss: 1.2184 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 1.0000e-04\n",
            "Epoch 7/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567ms/step - accuracy: 0.4313 - loss: 1.2090 - sparse_categorical_accuracy: 0.4313\n",
            "Epoch 7: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 653ms/step - accuracy: 0.4319 - loss: 1.2102 - sparse_categorical_accuracy: 0.4319 - val_accuracy: 0.4286 - val_loss: 1.1999 - val_sparse_categorical_accuracy: 0.4286 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584ms/step - accuracy: 0.4951 - loss: 1.1731 - sparse_categorical_accuracy: 0.4951\n",
            "Epoch 8: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 737ms/step - accuracy: 0.4932 - loss: 1.1739 - sparse_categorical_accuracy: 0.4932 - val_accuracy: 0.4286 - val_loss: 1.2167 - val_sparse_categorical_accuracy: 0.4286 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606ms/step - accuracy: 0.4335 - loss: 1.1227 - sparse_categorical_accuracy: 0.4335\n",
            "Epoch 9: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 759ms/step - accuracy: 0.4349 - loss: 1.1226 - sparse_categorical_accuracy: 0.4349 - val_accuracy: 0.4524 - val_loss: 1.2077 - val_sparse_categorical_accuracy: 0.4524 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602ms/step - accuracy: 0.5219 - loss: 0.9953 - sparse_categorical_accuracy: 0.5219\n",
            "Epoch 10: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 683ms/step - accuracy: 0.5224 - loss: 0.9978 - sparse_categorical_accuracy: 0.5224 - val_accuracy: 0.4524 - val_loss: 1.1738 - val_sparse_categorical_accuracy: 0.4524 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566ms/step - accuracy: 0.5279 - loss: 1.0754 - sparse_categorical_accuracy: 0.5279\n",
            "Epoch 11: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 644ms/step - accuracy: 0.5262 - loss: 1.0791 - sparse_categorical_accuracy: 0.5262 - val_accuracy: 0.4286 - val_loss: 1.2125 - val_sparse_categorical_accuracy: 0.4286 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519ms/step - accuracy: 0.4747 - loss: 1.0024 - sparse_categorical_accuracy: 0.4747\n",
            "Epoch 12: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 673ms/step - accuracy: 0.4750 - loss: 1.0028 - sparse_categorical_accuracy: 0.4750 - val_accuracy: 0.4524 - val_loss: 1.2361 - val_sparse_categorical_accuracy: 0.4524 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576ms/step - accuracy: 0.5379 - loss: 1.0653 - sparse_categorical_accuracy: 0.5379\n",
            "Epoch 13: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 658ms/step - accuracy: 0.5378 - loss: 1.0641 - sparse_categorical_accuracy: 0.5378 - val_accuracy: 0.4762 - val_loss: 1.1884 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582ms/step - accuracy: 0.5145 - loss: 1.0075 - sparse_categorical_accuracy: 0.5145\n",
            "Epoch 14: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 664ms/step - accuracy: 0.5141 - loss: 1.0102 - sparse_categorical_accuracy: 0.5141 - val_accuracy: 0.4762 - val_loss: 1.1866 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589ms/step - accuracy: 0.5896 - loss: 0.9933 - sparse_categorical_accuracy: 0.5896\n",
            "Epoch 15: val_accuracy did not improve from 0.47619\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 673ms/step - accuracy: 0.5880 - loss: 0.9925 - sparse_categorical_accuracy: 0.5880 - val_accuracy: 0.4286 - val_loss: 1.2019 - val_sparse_categorical_accuracy: 0.4286 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596ms/step - accuracy: 0.5331 - loss: 1.0083 - sparse_categorical_accuracy: 0.5331\n",
            "Epoch 16: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 678ms/step - accuracy: 0.5328 - loss: 1.0070 - sparse_categorical_accuracy: 0.5328 - val_accuracy: 0.4524 - val_loss: 1.1654 - val_sparse_categorical_accuracy: 0.4524 - learning_rate: 5.0000e-05\n",
            "Epoch 17/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605ms/step - accuracy: 0.6050 - loss: 0.9535 - sparse_categorical_accuracy: 0.6050\n",
            "Epoch 17: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 686ms/step - accuracy: 0.6037 - loss: 0.9514 - sparse_categorical_accuracy: 0.6037 - val_accuracy: 0.4524 - val_loss: 1.1394 - val_sparse_categorical_accuracy: 0.4524 - learning_rate: 5.0000e-05\n",
            "Epoch 18/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 583ms/step - accuracy: 0.5957 - loss: 0.8910 - sparse_categorical_accuracy: 0.5957\n",
            "Epoch 18: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 662ms/step - accuracy: 0.5951 - loss: 0.8910 - sparse_categorical_accuracy: 0.5951 - val_accuracy: 0.4762 - val_loss: 1.1555 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-05\n",
            "Epoch 19/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 582ms/step - accuracy: 0.6686 - loss: 0.7702 - sparse_categorical_accuracy: 0.6686\n",
            "Epoch 19: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 735ms/step - accuracy: 0.6646 - loss: 0.7762 - sparse_categorical_accuracy: 0.6646 - val_accuracy: 0.4762 - val_loss: 1.1685 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-05\n",
            "Epoch 20/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584ms/step - accuracy: 0.6158 - loss: 0.8704 - sparse_categorical_accuracy: 0.6158\n",
            "Epoch 20: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 669ms/step - accuracy: 0.6168 - loss: 0.8690 - sparse_categorical_accuracy: 0.6168 - val_accuracy: 0.4524 - val_loss: 1.1506 - val_sparse_categorical_accuracy: 0.4524 - learning_rate: 5.0000e-05\n",
            "Epoch 21/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 569ms/step - accuracy: 0.6370 - loss: 0.8577 - sparse_categorical_accuracy: 0.6370\n",
            "Epoch 21: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 651ms/step - accuracy: 0.6355 - loss: 0.8580 - sparse_categorical_accuracy: 0.6355 - val_accuracy: 0.4762 - val_loss: 1.1284 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-05\n",
            "Epoch 22/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545ms/step - accuracy: 0.5994 - loss: 0.9178 - sparse_categorical_accuracy: 0.5994\n",
            "Epoch 22: val_accuracy did not improve from 0.47619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 701ms/step - accuracy: 0.5985 - loss: 0.9170 - sparse_categorical_accuracy: 0.5985 - val_accuracy: 0.4286 - val_loss: 1.1100 - val_sparse_categorical_accuracy: 0.4286 - learning_rate: 5.0000e-05\n",
            "Epoch 23/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517ms/step - accuracy: 0.6593 - loss: 0.7956 - sparse_categorical_accuracy: 0.6593\n",
            "Epoch 23: val_accuracy improved from 0.47619 to 0.52381, saving model to models/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 669ms/step - accuracy: 0.6579 - loss: 0.7967 - sparse_categorical_accuracy: 0.6579 - val_accuracy: 0.5238 - val_loss: 1.1072 - val_sparse_categorical_accuracy: 0.5238 - learning_rate: 5.0000e-05\n",
            "Epoch 24/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579ms/step - accuracy: 0.5865 - loss: 0.7692 - sparse_categorical_accuracy: 0.5865\n",
            "Epoch 24: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 658ms/step - accuracy: 0.5867 - loss: 0.7719 - sparse_categorical_accuracy: 0.5867 - val_accuracy: 0.4762 - val_loss: 1.1271 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-05\n",
            "Epoch 25/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567ms/step - accuracy: 0.5529 - loss: 0.8243 - sparse_categorical_accuracy: 0.5529\n",
            "Epoch 25: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 651ms/step - accuracy: 0.5567 - loss: 0.8245 - sparse_categorical_accuracy: 0.5567 - val_accuracy: 0.4762 - val_loss: 1.1398 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-05\n",
            "Restoring model weights from the end of the best epoch: 23.\n",
            "\n",
            "[5/5] Entrenando modelo - Fase 2: Fine-tuning...\n",
            "Advertencia: No se encontró MobileNetV2, descongelando todo el modelo\n",
            "\n",
            "Modelo compilado con éxito\n",
            "Epoch 1/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6213 - loss: 0.7962 - sparse_categorical_accuracy: 0.6213\n",
            "Epoch 1: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.6211 - loss: 0.7981 - sparse_categorical_accuracy: 0.6211 - val_accuracy: 0.5238 - val_loss: 1.0915 - val_sparse_categorical_accuracy: 0.5238 - learning_rate: 1.0000e-05\n",
            "Epoch 2/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615ms/step - accuracy: 0.6555 - loss: 0.7975 - sparse_categorical_accuracy: 0.6555\n",
            "Epoch 2: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 768ms/step - accuracy: 0.6569 - loss: 0.7960 - sparse_categorical_accuracy: 0.6569 - val_accuracy: 0.5000 - val_loss: 1.0957 - val_sparse_categorical_accuracy: 0.5000 - learning_rate: 1.0000e-05\n",
            "Epoch 3/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 585ms/step - accuracy: 0.6475 - loss: 0.7806 - sparse_categorical_accuracy: 0.6475\n",
            "Epoch 3: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 663ms/step - accuracy: 0.6459 - loss: 0.7812 - sparse_categorical_accuracy: 0.6459 - val_accuracy: 0.4762 - val_loss: 1.1125 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 1.0000e-05\n",
            "Epoch 4/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588ms/step - accuracy: 0.6400 - loss: 0.8128 - sparse_categorical_accuracy: 0.6400\n",
            "Epoch 4: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 667ms/step - accuracy: 0.6398 - loss: 0.8126 - sparse_categorical_accuracy: 0.6398 - val_accuracy: 0.4762 - val_loss: 1.1109 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 1.0000e-05\n",
            "Epoch 5/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 579ms/step - accuracy: 0.6743 - loss: 0.8203 - sparse_categorical_accuracy: 0.6743\n",
            "Epoch 5: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 659ms/step - accuracy: 0.6731 - loss: 0.8200 - sparse_categorical_accuracy: 0.6731 - val_accuracy: 0.4762 - val_loss: 1.1160 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 1.0000e-05\n",
            "Epoch 6/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564ms/step - accuracy: 0.6731 - loss: 0.7725 - sparse_categorical_accuracy: 0.6731\n",
            "Epoch 6: val_accuracy did not improve from 0.52381\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 665ms/step - accuracy: 0.6704 - loss: 0.7745 - sparse_categorical_accuracy: 0.6704 - val_accuracy: 0.4762 - val_loss: 1.1043 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 1.0000e-05\n",
            "Epoch 7/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575ms/step - accuracy: 0.6588 - loss: 0.7430 - sparse_categorical_accuracy: 0.6588\n",
            "Epoch 7: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 730ms/step - accuracy: 0.6585 - loss: 0.7434 - sparse_categorical_accuracy: 0.6585 - val_accuracy: 0.4762 - val_loss: 1.1063 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-06\n",
            "Epoch 8/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - accuracy: 0.6268 - loss: 0.8356 - sparse_categorical_accuracy: 0.6268\n",
            "Epoch 8: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 803ms/step - accuracy: 0.6254 - loss: 0.8355 - sparse_categorical_accuracy: 0.6254 - val_accuracy: 0.4762 - val_loss: 1.1108 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-06\n",
            "Epoch 9/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555ms/step - accuracy: 0.5811 - loss: 0.8213 - sparse_categorical_accuracy: 0.5811\n",
            "Epoch 9: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 688ms/step - accuracy: 0.5825 - loss: 0.8177 - sparse_categorical_accuracy: 0.5825 - val_accuracy: 0.4762 - val_loss: 1.1105 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-06\n",
            "Epoch 10/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518ms/step - accuracy: 0.6556 - loss: 0.8344 - sparse_categorical_accuracy: 0.6556\n",
            "Epoch 10: val_accuracy did not improve from 0.52381\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 634ms/step - accuracy: 0.6568 - loss: 0.8329 - sparse_categorical_accuracy: 0.6568 - val_accuracy: 0.4762 - val_loss: 1.1096 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-06\n",
            "Epoch 11/25\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528ms/step - accuracy: 0.6249 - loss: 0.7690 - sparse_categorical_accuracy: 0.6249\n",
            "Epoch 11: val_accuracy did not improve from 0.52381\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 616ms/step - accuracy: 0.6253 - loss: 0.7688 - sparse_categorical_accuracy: 0.6253 - val_accuracy: 0.4762 - val_loss: 1.1102 - val_sparse_categorical_accuracy: 0.4762 - learning_rate: 5.0000e-06\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "============================================================\n",
            "EVALUACIÓN EN TEST SET\n",
            "============================================================\n",
            "\n",
            "Test Loss: 1.0569\n",
            "Test Accuracy: 0.5663\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Grade 0       0.85      0.81      0.83        27\n",
            "     Grade 1       0.20      0.25      0.22         4\n",
            "     Grade 2       0.50      0.37      0.43        27\n",
            "     Grade 3       0.44      0.53      0.48        15\n",
            "     Grade 4       0.43      0.60      0.50        10\n",
            "\n",
            "    accuracy                           0.57        83\n",
            "   macro avg       0.48      0.51      0.49        83\n",
            "weighted avg       0.58      0.57      0.57        83\n",
            "\n",
            "\n",
            "=== Confusion Matrix ===\n",
            "[[22  2  3  0  0]\n",
            " [ 0  1  2  1  0]\n",
            " [ 3  1 10  8  5]\n",
            " [ 1  1  2  8  3]\n",
            " [ 0  0  3  1  6]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gráfica guardada en: training_history.png\n",
            "\n",
            "============================================================\n",
            "ENTRENAMIENTO COMPLETADO\n",
            "============================================================\n",
            "Modelo guardado en: models/final_model.h5\n",
            "Mejor modelo en: models/best_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a5fbd7a"
      },
      "source": [
        "\n",
        "\n",
        "### Execution Code\n",
        "\n",
        "Paste your Python execution code here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4728156"
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def load_model(model_path='models/best_model.h5'):\n",
        "    \"\"\"Carga el modelo entrenado\"\"\"\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Error: No se encontró el modelo en {model_path}\")\n",
        "        return None\n",
        "\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    print(f\"✓ Modelo cargado desde: {model_path}\")\n",
        "    return model\n",
        "\n",
        "def preprocess_image(image_path, img_size=224):\n",
        "    \"\"\"Preprocesa una imagen para predicción\"\"\"\n",
        "    # Leer imagen\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"No se pudo leer la imagen: {image_path}\")\n",
        "\n",
        "    # Convertir a RGB\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Redimensionar\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "\n",
        "    # Normalizar (preprocesamiento de MobileNet)\n",
        "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "\n",
        "    # Añadir dimensión de batch\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    return img\n",
        "\n",
        "def predict_image(model, image_path):\n",
        "    \"\"\"Hace predicción en una sola imagen\"\"\"\n",
        "    # Preprocesar\n",
        "    img = preprocess_image(image_path)\n",
        "\n",
        "    # Predecir\n",
        "    prediction = model.predict(img, verbose=0)\n",
        "\n",
        "    # Obtener clase y confianza\n",
        "    grade = np.argmax(prediction)\n",
        "    confidence = np.max(prediction) * 100\n",
        "\n",
        "    # Todas las probabilidades\n",
        "    probabilities = prediction[0] * 100\n",
        "\n",
        "    return grade, confidence, probabilities\n",
        "\n",
        "def get_grade_description(grade):\n",
        "    \"\"\"Devuelve la descripción del grado de retinopatía\"\"\"\n",
        "    descriptions = {\n",
        "        0: \"Sin retinopatía aparente\",\n",
        "        1: \"Retinopatía no proliferativa leve\",\n",
        "        2: \"Retinopatía no proliferativa moderada\",\n",
        "        3: \"Retinopatía no proliferativa severa\",\n",
        "        4: \"Retinopatía proliferativa\"\n",
        "    }\n",
        "    return descriptions.get(grade, \"Desconocido\")\n",
        "\n",
        "def predict_single_image(model_path, image_path):\n",
        "    \"\"\"Predice en una sola imagen y muestra resultados\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PREDICCIÓN DE RETINOPATÍA DIABÉTICA\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Cargar modelo\n",
        "    model = load_model(model_path)\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # Verificar imagen\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"Error: No se encontró la imagen: {image_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nImagen: {image_path}\")\n",
        "\n",
        "    # Hacer predicción\n",
        "    try:\n",
        "        grade, confidence, probabilities = predict_image(model, image_path)\n",
        "\n",
        "        print(\"\\n--- RESULTADO ---\")\n",
        "        print(f\"Grado predicho: {grade}\")\n",
        "        print(f\"Diagnóstico: {get_grade_description(grade)}\")\n",
        "        print(f\"Confianza: {confidence:.2f}%\")\n",
        "\n",
        "        print(\"\\n--- PROBABILIDADES POR CLASE ---\")\n",
        "        for i, prob in enumerate(probabilities):\n",
        "            print(f\"  Grado {i}: {prob:.2f}% - {get_grade_description(i)}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error durante la predicción: {str(e)}\")\n",
        "\n",
        "def predict_batch(model_path, images_dir):\n",
        "    \"\"\"Predice en múltiples imágenes de un directorio\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PREDICCIÓN BATCH - RETINOPATÍA DIABÉTICA\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Cargar modelo\n",
        "    model = load_model(model_path)\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # Buscar imágenes\n",
        "    extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']\n",
        "    image_files = [f for f in os.listdir(images_dir)\n",
        "                   if any(f.endswith(ext) for ext in extensions)]\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"No se encontraron imágenes en: {images_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nEncontradas {len(image_files)} imágenes\")\n",
        "    print(\"\\nProcesando...\\n\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(images_dir, image_file)\n",
        "\n",
        "        try:\n",
        "            grade, confidence, probabilities = predict_image(model, image_path)\n",
        "\n",
        "            results.append({\n",
        "                'image': image_file,\n",
        "                'grade': grade,\n",
        "                'confidence': confidence,\n",
        "                'diagnosis': get_grade_description(grade)\n",
        "            })\n",
        "\n",
        "            print(f\"✓ {image_file:30s} -> Grado {grade} ({confidence:.1f}%)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ {image_file:30s} -> Error: {str(e)}\")\n",
        "\n",
        "    # Resumen\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"RESUMEN\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if results:\n",
        "        print(f\"\\nImágenes procesadas: {len(results)}\")\n",
        "        print(\"\\nDistribución de grados:\")\n",
        "\n",
        "        from collections import Counter\n",
        "        grade_counts = Counter([r['grade'] for r in results])\n",
        "        for grade in sorted(grade_counts.keys()):\n",
        "            count = grade_counts[grade]\n",
        "            percentage = (count / len(results)) * 100\n",
        "            print(f\"  Grado {grade}: {count} imágenes ({percentage:.1f}%)\")\n",
        "\n",
        "        # Confianza promedio\n",
        "        avg_confidence = np.mean([r['confidence'] for r in results])\n",
        "        print(f\"\\nConfianza promedio: {avg_confidence:.2f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"Función principal\"\"\"\n",
        "#     import argparse\n",
        "\n",
        "#     parser = argparse.ArgumentParser(description='Predicción de Retinopatía Diabética')\n",
        "#     parser.add_argument('--model', type=str, default='models/best_model.h5',\n",
        "#                         help='Ruta al modelo entrenado')\n",
        "#     parser.add_argument('--image', type=str, help='Ruta a una imagen individual')\n",
        "#     parser.add_argument('--batch', type=str, help='Ruta a directorio con imágenes')\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     if args.image:\n",
        "#         predict_single_image(args.model, args.image)\n",
        "#     elif args.batch:\n",
        "#         predict_batch(args.model, args.batch)\n",
        "#     else:\n",
        "#         print(\"Uso:\")\n",
        "#         print(\"  Predicción individual: python predict.py --image ruta/imagen.jpg\")\n",
        "#         print(\"  Predicción batch:      python predict.py --batch ruta/directorio\")\n",
        "#         print(\"  Especificar modelo:    python predict.py --model models/final_model.h5 --image imagen.jpg\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n",
        "\n",
        "# Para usar en Colab, llama directamente a las funciones. Por ejemplo:\n",
        "# predict_single_image(model_path='models/best_model.h5', image_path='/content/data/test_images/image1.jpg')\n",
        "# predict_batch(model_path='models/best_model.h5', images_dir='/content/data/test_images/')\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pjQ7fS4aCHHb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}